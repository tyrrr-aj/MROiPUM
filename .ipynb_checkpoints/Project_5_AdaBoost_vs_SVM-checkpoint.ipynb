{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Porównanie skuteczności klasyfikatorów SVM i AdaBoost na zbiorach danych o wymiarowości zredukowanej przez autoenkoder wariacyjny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przygotowanie zbioru danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
      "0      2       0       0       0       0       0       0       0       0   \n",
      "1      9       0       0       0       0       0       0       0       0   \n",
      "2      6       0       0       0       0       0       0       0       5   \n",
      "3      0       0       0       0       1       2       0       0       0   \n",
      "4      3       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
      "0       0  ...         0         0         0         0         0         0   \n",
      "1       0  ...         0         0         0         0         0         0   \n",
      "2       0  ...         0         0         0        30        43         0   \n",
      "3       0  ...         3         0         0         0         0         1   \n",
      "4       0  ...         0         0         0         0         0         0   \n",
      "\n",
      "   pixel781  pixel782  pixel783  pixel784  \n",
      "0         0         0         0         0  \n",
      "1         0         0         0         0  \n",
      "2         0         0         0         0  \n",
      "3         0         0         0         0  \n",
      "4         0         0         0         0  \n",
      "\n",
      "[5 rows x 785 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with zipfile.ZipFile(\"data/fashion-mnist.zip\") as z:\n",
    "   with z.open(\"fashion-mnist_train.csv\") as f:\n",
    "      train_df = pd.read_csv(f)\n",
    "      print(train_df.head())    # print the first 5 rows\n",
    "\n",
    "   with z.open(\"fashion-mnist_test.csv\") as f:\n",
    "      test_df = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redukcja wymiarów za pomocą autoenkodera wariacyjnego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is OS\n",
      " Volume Serial Number is 2EF0-0992\n",
      "\n",
      " Directory of C:\\Users\\sgica\\Desktop\\mro_project\\MROiPUM\n",
      "\n",
      "11.06.2020  12:27    <DIR>          .\n",
      "11.06.2020  12:27    <DIR>          ..\n",
      "08.06.2020  19:35                65 .gitattributes\n",
      "11.06.2020  00:21    <DIR>          .ipynb_checkpoints\n",
      "11.06.2020  00:21    <DIR>          data\n",
      "11.06.2020  12:27            22˙077 Project_5_AdaBoost_vs_SVM.ipynb\n",
      "08.06.2020  19:35                 9 README.md\n",
      "               3 File(s)         22˙151 bytes\n",
      "               4 Dir(s)  23˙823˙413˙248 bytes free\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.2.0-cp38-cp38-win_amd64.whl (459.2 MB)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.29.0-cp38-cp38-win_amd64.whl (2.4 MB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in c:\\users\\sgica\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow) (1.18.2)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.2.1-py3-none-any.whl (63 kB)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in c:\\users\\sgica\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow) (3.12.2)\n",
      "Collecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in c:\\users\\sgica\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Collecting tensorboard<2.3.0,>=2.2.0\n",
      "  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Downloading absl-py-0.9.0.tar.gz (104 kB)\n",
      "Collecting wrapt>=1.11.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting wheel>=0.26; python_version >= \"3\"\n",
      "  Using cached wheel-0.34.2-py2.py3-none-any.whl (26 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in c:\\users\\sgica\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\sgica\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in c:\\users\\sgica\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow) (1.4.1)\n",
      "Collecting google-pasta>=0.1.8\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
      "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sgica\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from protobuf>=3.8.0->tensorflow) (41.2.0)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.6.0.post3-py3-none-any.whl (777 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\sgica\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (2.23.0)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.17.0-py2.py3-none-any.whl (90 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.2.2-py3-none-any.whl (88 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\sgica\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\sgica\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\sgica\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sgica\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2020.4.5.1)\n",
      "Collecting rsa<5.0,>=3.1.4\n",
      "  Downloading rsa-4.1-py3-none-any.whl (32 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.1.0-py3-none-any.whl (10 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Using legacy setup.py install for absl-py, since package 'wheel' is not installed.\n",
      "Using legacy setup.py install for wrapt, since package 'wheel' is not installed.\n",
      "Using legacy setup.py install for termcolor, since package 'wheel' is not installed.\n",
      "Installing collected packages: grpcio, opt-einsum, wheel, astunparse, werkzeug, oauthlib, requests-oauthlib, pyasn1, rsa, pyasn1-modules, cachetools, google-auth, google-auth-oauthlib, tensorboard-plugin-wit, absl-py, markdown, tensorboard, wrapt, termcolor, gast, google-pasta, tensorflow-estimator, tensorflow\n",
      "    Running setup.py install for absl-py: started\n",
      "    Running setup.py install for absl-py: finished with status 'done'\n",
      "    Running setup.py install for wrapt: started\n",
      "    Running setup.py install for wrapt: finished with status 'done'\n",
      "    Running setup.py install for termcolor: started\n",
      "    Running setup.py install for termcolor: finished with status 'done'\n",
      "Successfully installed absl-py-0.9.0 astunparse-1.6.3 cachetools-4.1.0 gast-0.3.3 google-auth-1.17.0 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.29.0 markdown-3.2.2 oauthlib-3.1.0 opt-einsum-3.2.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.1 tensorboard-2.2.2 tensorboard-plugin-wit-1.6.0.post3 tensorflow-2.2.0 tensorflow-estimator-2.2.0 termcolor-1.1.0 werkzeug-1.0.1 wheel-0.34.2 wrapt-1.12.1\n"
     ]
    }
   ],
   "source": [
    "#! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import objectives\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przygotowanie modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    z_mean, z_variance = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, dest_dim))\n",
    "    \n",
    "    return z_mean + K.exp(z_variance) * epsilon\n",
    "\n",
    "def get_intermediate_dims(initial_dim, dest_dim):\n",
    "    intermediate_dim_1 = int((initial_dim + dest_dim) / 2)\n",
    "    intermediate_dim_2 = int((intermediate_dim_1 + dest_dim) / 2)\n",
    "    \n",
    "    return intermediate_dim_1, intermediate_dim_2\n",
    "\n",
    "def create_architecture(batch_size, initial_dim, intermediate_dim_1, intermediate_dim_2, dest_dim):\n",
    "    input_layer = Input(batch_shape=(batch_size, initial_dim))\n",
    "\n",
    "    encoder_l1 = Dense(intermediate_dim_1, activation='relu')(input_layer)\n",
    "    encoder_l2 = Dense(intermediate_dim_2, activation='relu')(encoder_l1)\n",
    "\n",
    "    z_mean = Dense(dest_dim)(encoder_l2)\n",
    "    z_variance = Dense(dest_dim)(encoder_l2)\n",
    "\n",
    "    reduced = Lambda(sampling)([z_mean, z_variance])\n",
    "\n",
    "    decoder_l1 = Dense(intermediate_dim_2, activation='relu')(reduced)\n",
    "    decoder_l2 = Dense(intermediate_dim_1, activation='relu')(decoder_l1)\n",
    "\n",
    "    decoded = Dense(initial_dim, activation='sigmoid')(decoder_l2)\n",
    "    \n",
    "    return input_layer, reduced, decoded, z_mean, z_variance\n",
    "\n",
    "def vae_loss(x, x_decoded_mean):\n",
    "    xent_loss = objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.mean(1 + z_variance - K.square(z_mean) - K.exp(z_variance), axis=-1)\n",
    "    return xent_loss + kl_loss\n",
    "\n",
    "def prepare_models(input_layer, reduced, decoded):\n",
    "    end_to_end = Model(input_layer, decoded)\n",
    "    encoder = Model(input_layer, reduced)\n",
    "    \n",
    "    return end_to_end, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(batch_size, initial_dim, dest_dim):\n",
    "    intermediate_dim_1, intermediate_dim_2 = get_intermediate_dims(initial_dim, dest_dim)\n",
    "    input_layer, reduced, decoded, z_mean, z_variance = create_architecture(batch_size,\n",
    "                                                                            initial_dim,\n",
    "                                                                            intermediate_dim_1,\n",
    "                                                                            intermediate_dim_2,\n",
    "                                                                            dest_dim\n",
    "                                                                           )\n",
    "    end_to_end, encoder = prepare_models(input_layer, reduced, decoded)\n",
    "    return end_to_end, encoder, z_mean, z_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uruchomienie modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 9 6 ... 8 8 7]\n",
      "[0 1 2 ... 8 8 1]\n"
     ]
    }
   ],
   "source": [
    "train_labels = train_df['label'].to_numpy()\n",
    "test_labels = test_df['label'].to_numpy()\n",
    "print(train_labels)\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_df.drop('label', axis='columns').to_numpy() / 255.0\n",
    "test = test_df.drop('label', axis='columns').to_numpy() / 255.0\n",
    "\n",
    "batch_size = 10000\n",
    "initial_dim = len(train[0])  # =784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.8761 - val_loss: 0.5379\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 0.5259 - val_loss: 0.5089\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.5021 - val_loss: 0.4982\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 8s 128us/step - loss: 0.8342 - val_loss: 0.5398\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 99us/step - loss: 0.5216 - val_loss: 0.5047\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.5012 - val_loss: 0.5002\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.4989 - val_loss: 0.4983\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 103us/step - loss: 0.5000 - val_loss: 0.4995\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.4965 - val_loss: 0.4945\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.4958 - val_loss: 0.4939\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.4971 - val_loss: 0.4946\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.4938 - val_loss: 0.4937\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.4943 - val_loss: 0.4927\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 8s 134us/step - loss: 0.9278 - val_loss: 0.5491\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.5253 - val_loss: 0.5100\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.5051 - val_loss: 0.5016\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.5028 - val_loss: 0.4979\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 6s 99us/step - loss: 0.4975 - val_loss: 0.4999\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 6s 99us/step - loss: 0.5014 - val_loss: 0.5016\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 6s 99us/step - loss: 0.4972 - val_loss: 0.4981\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 6s 102us/step - loss: 0.4980 - val_loss: 0.4965\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.4950 - val_loss: 0.4935\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 6s 99us/step - loss: 0.4945 - val_loss: 0.4939\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.4947 - val_loss: 0.4921\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 6s 99us/step - loss: 0.4925 - val_loss: 0.4938\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 6s 99us/step - loss: 0.4942 - val_loss: 0.4922\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.4921 - val_loss: 0.4916\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 6s 99us/step - loss: 0.4917 - val_loss: 0.4917\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 6s 99us/step - loss: 0.4927 - val_loss: 0.4907\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 6s 99us/step - loss: 0.4909 - val_loss: 0.4906\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.4916 - val_loss: 0.4912\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.4910 - val_loss: 0.4914\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 6s 99us/step - loss: 0.4915 - val_loss: 0.4904\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 9s 145us/step - loss: 0.6657 - val_loss: 0.5094\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 7s 111us/step - loss: 0.5029 - val_loss: 0.4975\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 7s 116us/step - loss: 0.4967 - val_loss: 0.4956\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 8s 134us/step - loss: 0.6606 - val_loss: 0.5091\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 0.5024 - val_loss: 0.4967\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 7s 116us/step - loss: 0.4966 - val_loss: 0.4961\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 0.4956 - val_loss: 0.4941\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 0.4950 - val_loss: 0.4938\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.4940 - val_loss: 0.4926\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 0.4931 - val_loss: 0.4937\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.4937 - val_loss: 0.4906\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.4925 - val_loss: 0.4925\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 0.4914 - val_loss: 0.4902\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 8s 134us/step - loss: 0.6544 - val_loss: 0.5101\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.5031 - val_loss: 0.4973\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 0.4967 - val_loss: 0.4954\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.4955 - val_loss: 0.4941\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 0.4951 - val_loss: 0.4936\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.4939 - val_loss: 0.4929\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.4935 - val_loss: 0.4939\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.4931 - val_loss: 0.4925\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.4933 - val_loss: 0.4909\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 0.4922 - val_loss: 0.4911\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 7s 111us/step - loss: 0.4920 - val_loss: 0.4899\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 0.4914 - val_loss: 0.4890\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 0.4906 - val_loss: 0.4890\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.4902 - val_loss: 0.4892\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 7s 109us/step - loss: 0.4894 - val_loss: 0.4869\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.4873 - val_loss: 0.4863\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.4851 - val_loss: 0.4844\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.4845 - val_loss: 0.4809\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.4829 - val_loss: 0.4798\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.4813 - val_loss: 0.4813\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.5975 - val_loss: 0.5061\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 8s 130us/step - loss: 0.5020 - val_loss: 0.5005\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 8s 130us/step - loss: 0.4978 - val_loss: 0.4966\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 10s 163us/step - loss: 0.6037 - val_loss: 0.5064\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 8s 133us/step - loss: 0.5016 - val_loss: 0.4987\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 8s 130us/step - loss: 0.4983 - val_loss: 0.4963\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 8s 136us/step - loss: 0.4954 - val_loss: 0.4976\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 8s 130us/step - loss: 0.4934 - val_loss: 0.4917\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 8s 130us/step - loss: 0.4891 - val_loss: 0.4889\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 8s 131us/step - loss: 0.4858 - val_loss: 0.4753\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 8s 130us/step - loss: 0.4766 - val_loss: 0.4728\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 8s 135us/step - loss: 0.4706 - val_loss: 0.4509\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 8s 131us/step - loss: 0.4499 - val_loss: 0.4385\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.5991 - val_loss: 0.5067\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 8s 131us/step - loss: 0.5025 - val_loss: 0.4990\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 8s 130us/step - loss: 0.4979 - val_loss: 0.4964\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 8s 130us/step - loss: 0.4954 - val_loss: 0.4974\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 8s 135us/step - loss: 0.4912 - val_loss: 0.4936\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 8s 133us/step - loss: 0.4890 - val_loss: 0.4801\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 9s 143us/step - loss: 0.4820 - val_loss: 0.4779\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 8s 129us/step - loss: 0.4767 - val_loss: 0.4705\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 8s 129us/step - loss: 0.4703 - val_loss: 0.4658\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 8s 130us/step - loss: 0.4565 - val_loss: 0.4616\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 8s 131us/step - loss: 0.4539 - val_loss: 0.4582\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 8s 131us/step - loss: 0.4352 - val_loss: 0.4254\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 8s 130us/step - loss: 0.4284 - val_loss: 0.4245\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 8s 131us/step - loss: 0.4276 - val_loss: 0.4183\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 8s 133us/step - loss: 0.4260 - val_loss: 0.4205\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 8s 130us/step - loss: 0.4203 - val_loss: 0.4169\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 8s 134us/step - loss: 0.4176 - val_loss: 0.4201\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 8s 134us/step - loss: 0.4202 - val_loss: 0.4108\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 8s 129us/step - loss: 0.4209 - val_loss: 0.4087\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 8s 129us/step - loss: 0.4122 - val_loss: 0.4129\n"
     ]
    }
   ],
   "source": [
    "dest_dims = [10, 50, 200]\n",
    "epoch_numbers = [3, 10, 20]\n",
    "\n",
    "reduced_datasets = {dest_dim: {epochs: (None, None) for epochs in epoch_numbers} for dest_dim in dest_dims}\n",
    "\n",
    "for dest_dim in dest_dims:\n",
    "    for epochs in epoch_numbers:\n",
    "        \n",
    "        end_to_end, encoder, z_mean, z_variance = create_model(batch_size, initial_dim, dest_dim)\n",
    "\n",
    "        end_to_end.compile(optimizer='rmsprop', loss=vae_loss)\n",
    "\n",
    "        end_to_end.fit(train, train,\n",
    "                       shuffle=True,\n",
    "                       epochs=epochs,\n",
    "                       batch_size=batch_size,\n",
    "                       validation_data=(test, test)\n",
    "                      )\n",
    "        \n",
    "        reduced_train = encoder.predict(train, batch_size=batch_size)\n",
    "        reduced_test = encoder.predict(test, batch_size=batch_size)\n",
    "        \n",
    "        reduced_datasets[dest_dim][epochs] = (reduced_train, reduced_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[-1.4664369  -0.5863899  -1.1907116  ...  0.07863773 -0.41145048\n",
      "  -1.7507269 ]\n",
      " [-0.6801017  -0.36279756 -0.76092243 ...  0.4665662   1.566969\n",
      "  -0.43449494]\n",
      " [-0.673329   -0.95815694  0.15312992 ...  1.3328222   0.96151054\n",
      "   0.29297107]\n",
      " ...\n",
      " [-0.54973376 -0.21949144  0.0568223  ...  1.1584927   1.9212215\n",
      "   0.91002655]\n",
      " [-0.11077318  0.53993016 -0.45462328 ...  1.4096466   0.13485791\n",
      "  -0.44492483]\n",
      " [-0.6048826  -1.7758096   1.137357   ... -1.2991505  -1.164543\n",
      "  -0.02538302]]\n"
     ]
    }
   ],
   "source": [
    "print(train)\n",
    "print(reduced_datasets[10][3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-0c282fe33e66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mada_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mada_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mytest_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mada_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mada_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'test_labels' is not defined"
     ]
    }
   ],
   "source": [
    "ada_model = AdaBoostClassifier()\n",
    "ada_model = ada_model.fit(train, train_labels)\n",
    "test_pred = ada_model.predict(test)\n",
    "print(ada_model.score(test, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5612\n"
     ]
    }
   ],
   "source": [
    "print(ada_model.score(test, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurracy for dimension 10 and 3 epochs is 0.1333\n",
      "Acurracy for dimension 10 and 10 epochs is 0.1381\n",
      "Acurracy for dimension 10 and 20 epochs is 0.1347\n",
      "Acurracy for dimension 50 and 3 epochs is 0.1283\n",
      "Acurracy for dimension 50 and 10 epochs is 0.1643\n",
      "Acurracy for dimension 50 and 20 epochs is 0.1913\n",
      "Acurracy for dimension 200 and 3 epochs is 0.1605\n",
      "Acurracy for dimension 200 and 10 epochs is 0.2858\n",
      "Acurracy for dimension 200 and 20 epochs is 0.3370\n"
     ]
    }
   ],
   "source": [
    "for dest_dim in dest_dims:\n",
    "    for epochs in epoch_numbers:\n",
    "        ada_model = AdaBoostClassifier()\n",
    "        ada_model = ada_model.fit(reduced_datasets[dest_dim][epochs][0], train_labels)\n",
    "        test_pred = ada_model.predict(reduced_datasets[dest_dim][epochs][1])\n",
    "        print('Acurracy for dimension %d and %d epochs is %.4f' % (dest_dim, epochs, ada_model.score(reduced_datasets[dest_dim][epochs][1], test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.83      0.80      1000\n",
      "           1       0.99      0.96      0.98      1000\n",
      "           2       0.81      0.77      0.79      1000\n",
      "           3       0.85      0.91      0.88      1000\n",
      "           4       0.79      0.83      0.81      1000\n",
      "           5       0.95      0.92      0.93      1000\n",
      "           6       0.66      0.59      0.62      1000\n",
      "           7       0.91      0.90      0.91      1000\n",
      "           8       0.96      0.96      0.96      1000\n",
      "           9       0.91      0.95      0.93      1000\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "svclassifier = SVC()\n",
    "svclassifier.fit(train[:10000], train_labels[:10000])\n",
    "y_pred = svclassifier.predict(test[:10000])\n",
    "#print(confusion_matrix(test_labels[:1000], y_pred[:1000]))\n",
    "print(classification_report(test_labels[:10000], y_pred[:10000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.86      0.84      1000\n",
      "           1       0.99      0.97      0.98      1000\n",
      "           2       0.84      0.82      0.83      1000\n",
      "           3       0.89      0.92      0.90      1000\n",
      "           4       0.84      0.87      0.85      1000\n",
      "           5       0.97      0.94      0.95      1000\n",
      "           6       0.74      0.68      0.71      1000\n",
      "           7       0.92      0.94      0.93      1000\n",
      "           8       0.97      0.97      0.97      1000\n",
      "           9       0.95      0.95      0.95      1000\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "svclassifier = SVC()\n",
    "svclassifier.fit(train, train_labels)\n",
    "y_pred = svclassifier.predict(test)\n",
    "print(classification_report(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Classification for dimension 10 and 3 epochs -----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.11      0.09      0.10      1000\n",
      "           1       0.15      0.17      0.16      1000\n",
      "           2       0.13      0.22      0.17      1000\n",
      "           3       0.13      0.12      0.12      1000\n",
      "           4       0.12      0.10      0.11      1000\n",
      "           5       0.13      0.09      0.11      1000\n",
      "           6       0.11      0.08      0.09      1000\n",
      "           7       0.16      0.14      0.15      1000\n",
      "           8       0.13      0.20      0.15      1000\n",
      "           9       0.11      0.08      0.09      1000\n",
      "\n",
      "    accuracy                           0.13     10000\n",
      "   macro avg       0.13      0.13      0.13     10000\n",
      "weighted avg       0.13      0.13      0.13     10000\n",
      "\n",
      "----- Classification for dimension 10 and 10 epochs -----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.07      0.08      1000\n",
      "           1       0.16      0.19      0.17      1000\n",
      "           2       0.12      0.12      0.12      1000\n",
      "           3       0.15      0.15      0.15      1000\n",
      "           4       0.11      0.06      0.08      1000\n",
      "           5       0.13      0.10      0.12      1000\n",
      "           6       0.10      0.07      0.08      1000\n",
      "           7       0.14      0.14      0.14      1000\n",
      "           8       0.14      0.33      0.20      1000\n",
      "           9       0.10      0.07      0.08      1000\n",
      "\n",
      "    accuracy                           0.13     10000\n",
      "   macro avg       0.13      0.13      0.12     10000\n",
      "weighted avg       0.13      0.13      0.12     10000\n",
      "\n",
      "----- Classification for dimension 10 and 20 epochs -----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.11      0.09      0.10      1000\n",
      "           1       0.19      0.23      0.21      1000\n",
      "           2       0.12      0.15      0.14      1000\n",
      "           3       0.13      0.12      0.13      1000\n",
      "           4       0.10      0.05      0.07      1000\n",
      "           5       0.10      0.07      0.08      1000\n",
      "           6       0.11      0.07      0.08      1000\n",
      "           7       0.12      0.11      0.12      1000\n",
      "           8       0.15      0.35      0.21      1000\n",
      "           9       0.12      0.09      0.10      1000\n",
      "\n",
      "    accuracy                           0.13     10000\n",
      "   macro avg       0.13      0.13      0.12     10000\n",
      "weighted avg       0.13      0.13      0.12     10000\n",
      "\n",
      "----- Classification for dimension 50 and 3 epochs -----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.11      0.11      0.11      1000\n",
      "           1       0.15      0.17      0.16      1000\n",
      "           2       0.11      0.12      0.12      1000\n",
      "           3       0.12      0.11      0.11      1000\n",
      "           4       0.10      0.09      0.10      1000\n",
      "           5       0.11      0.10      0.11      1000\n",
      "           6       0.11      0.09      0.10      1000\n",
      "           7       0.13      0.14      0.13      1000\n",
      "           8       0.13      0.17      0.15      1000\n",
      "           9       0.11      0.10      0.11      1000\n",
      "\n",
      "    accuracy                           0.12     10000\n",
      "   macro avg       0.12      0.12      0.12     10000\n",
      "weighted avg       0.12      0.12      0.12     10000\n",
      "\n",
      "----- Classification for dimension 50 and 10 epochs -----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.13      0.11      0.12      1000\n",
      "           1       0.23      0.28      0.26      1000\n",
      "           2       0.15      0.14      0.15      1000\n",
      "           3       0.17      0.19      0.18      1000\n",
      "           4       0.13      0.11      0.12      1000\n",
      "           5       0.13      0.12      0.13      1000\n",
      "           6       0.11      0.09      0.10      1000\n",
      "           7       0.19      0.21      0.20      1000\n",
      "           8       0.29      0.41      0.34      1000\n",
      "           9       0.14      0.11      0.12      1000\n",
      "\n",
      "    accuracy                           0.18     10000\n",
      "   macro avg       0.17      0.18      0.17     10000\n",
      "weighted avg       0.17      0.18      0.17     10000\n",
      "\n",
      "----- Classification for dimension 50 and 20 epochs -----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.14      0.14      0.14      1000\n",
      "           1       0.42      0.53      0.47      1000\n",
      "           2       0.19      0.23      0.21      1000\n",
      "           3       0.22      0.23      0.22      1000\n",
      "           4       0.15      0.12      0.13      1000\n",
      "           5       0.17      0.16      0.17      1000\n",
      "           6       0.13      0.09      0.11      1000\n",
      "           7       0.27      0.29      0.28      1000\n",
      "           8       0.24      0.29      0.26      1000\n",
      "           9       0.15      0.12      0.13      1000\n",
      "\n",
      "    accuracy                           0.22     10000\n",
      "   macro avg       0.21      0.22      0.21     10000\n",
      "weighted avg       0.21      0.22      0.21     10000\n",
      "\n",
      "----- Classification for dimension 200 and 3 epochs -----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.13      0.12      0.13      1000\n",
      "           1       0.28      0.33      0.30      1000\n",
      "           2       0.20      0.20      0.20      1000\n",
      "           3       0.20      0.21      0.21      1000\n",
      "           4       0.16      0.15      0.15      1000\n",
      "           5       0.16      0.17      0.16      1000\n",
      "           6       0.12      0.10      0.11      1000\n",
      "           7       0.23      0.24      0.24      1000\n",
      "           8       0.24      0.26      0.25      1000\n",
      "           9       0.19      0.18      0.18      1000\n",
      "\n",
      "    accuracy                           0.20     10000\n",
      "   macro avg       0.19      0.20      0.19     10000\n",
      "weighted avg       0.19      0.20      0.19     10000\n",
      "\n",
      "----- Classification for dimension 200 and 10 epochs -----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.35      0.32      1000\n",
      "           1       0.64      0.70      0.67      1000\n",
      "           2       0.28      0.34      0.31      1000\n",
      "           3       0.41      0.43      0.42      1000\n",
      "           4       0.22      0.19      0.20      1000\n",
      "           5       0.42      0.36      0.39      1000\n",
      "           6       0.19      0.14      0.17      1000\n",
      "           7       0.59      0.66      0.62      1000\n",
      "           8       0.34      0.33      0.33      1000\n",
      "           9       0.49      0.45      0.47      1000\n",
      "\n",
      "    accuracy                           0.40     10000\n",
      "   macro avg       0.39      0.40      0.39     10000\n",
      "weighted avg       0.39      0.40      0.39     10000\n",
      "\n",
      "----- Classification for dimension 200 and 20 epochs -----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.54      0.50      1000\n",
      "           1       0.63      0.72      0.67      1000\n",
      "           2       0.33      0.36      0.34      1000\n",
      "           3       0.41      0.40      0.40      1000\n",
      "           4       0.33      0.31      0.32      1000\n",
      "           5       0.49      0.43      0.46      1000\n",
      "           6       0.21      0.15      0.18      1000\n",
      "           7       0.62      0.65      0.63      1000\n",
      "           8       0.50      0.51      0.50      1000\n",
      "           9       0.70      0.70      0.70      1000\n",
      "\n",
      "    accuracy                           0.48     10000\n",
      "   macro avg       0.47      0.48      0.47     10000\n",
      "weighted avg       0.47      0.48      0.47     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dest_dim in dest_dims:\n",
    "    for epochs in epoch_numbers:\n",
    "        svclassifier = SVC()\n",
    "        svclassifier.fit(reduced_datasets[dest_dim][epochs][0], train_labels)\n",
    "        y_pred = svclassifier.predict(reduced_datasets[dest_dim][epochs][1])\n",
    "        print('----- Classification for dimension %d and %d epochs -----' % (dest_dim, epochs))\n",
    "        print(classification_report(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Źródła"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://towardsdatascience.com/auto-encoder-what-is-it-and-what-is-it-used-for-part-1-3e5c6f017726\n",
    "* https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73\n",
    "* https://blog.keras.io/building-autoencoders-in-keras.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
